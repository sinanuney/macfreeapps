from flask import Flask, jsonify, request, send_from_directory
from flask_cors import CORS
import json
import os
from math import ceil
from datetime import datetime, timezone
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import re
import time
import random

app = Flask(__name__)
CORS(app, resources={r"/api/*": {"origins": "*"}})

# Kategori hiyerarÅŸisi - App Store kategorileri
CATEGORY_HIERARCHY = {
    "Oyunlar": {
        "name": "Oyunlar",
        "subcategories": [
            "Aksiyon", "Macera", "Masa OyunlarÄ±", "Kart OyunlarÄ±", 
            "Kumarhane OyunlarÄ±", "Basit EÄŸlence", "Aile", "MÃ¼zik OyunlarÄ±",
            "Bulmaca", "YarÄ±ÅŸ", "Rol Yapma", "SimÃ¼lasyon", "Spor OyunlarÄ±",
            "Strateji", "Bilgi YarÄ±ÅŸmasÄ±", "Kelime OyunlarÄ±"
        ]
    },
    "Programlar": {
        "name": "Programlar",
        "subcategories": [
            "Ä°ÅŸ", "GeliÅŸtirici AraÃ§larÄ±", "EÄŸitim", "EÄŸlence", "Finans",
            "Grafik ve TasarÄ±m", "SaÄŸlÄ±k ve Fitness", "YaÅŸam TarzÄ±", "TÄ±p",
            "MÃ¼zik", "Haberler", "FotoÄŸraf ve Video", "Verimlilik", "Referans",
            "AlÄ±ÅŸveriÅŸ", "Sosyal AÄŸ", "Spor", "Seyahat", "YardÄ±mcÄ± Programlar",
            "Hava Durumu"
        ]
    }
}

# TÃ¼m kategorileri dÃ¼z liste halinde al
def get_all_categories():
    all_categories = ["Kategorisiz"]
    for main_cat, data in CATEGORY_HIERARCHY.items():
        all_categories.append(main_cat)
        all_categories.extend(data["subcategories"])
    return all_categories

APPS_FILE = 'apps.json'

def load_apps():
    """apps.json dosyasÄ±ndan uygulamalarÄ± yÃ¼kler."""
    if not os.path.exists(APPS_FILE):
        return []
    try:
        with open(APPS_FILE, 'r', encoding='utf-8') as f:
            # DosyanÄ±n boÅŸ olup olmadÄ±ÄŸÄ±nÄ± kontrol et
            content = f.read()
            if not content:
                return []
            return json.loads(content)
    except (json.JSONDecodeError, FileNotFoundError):
        return []

def save_apps(apps):
    """Uygulama listesini apps.json dosyasÄ±na kaydeder."""
    with open(APPS_FILE, 'w', encoding='utf-8') as f:
        json.dump(apps, f, indent=4, ensure_ascii=False)

def find_by_keyword(soup, keywords, tag_name='div'):
    """Belirli anahtar kelimeleri iÃ§eren metinleri arar."""
    for keyword in keywords:
        try:
            element = soup.find(lambda tag: tag.name == tag_name and keyword in tag.get_text(strip=True).lower())
            if element:
                parent_text = element.get_text(separator=' ', strip=True)
                match = re.search(fr'{re.escape(keyword)}[\s:]*([\d.v\sA-Z]+[a-zA-Z]?)?', parent_text, re.IGNORECASE)
                if match and match.group(1):
                    return match.group(1).strip()
        except Exception:
            continue
    return None

def get_highest_res_from_srcset(srcset):
    """srcset iÃ§indeki en yÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ gÃ¶rseli bulur."""
    if not srcset:
        return None
    
    highest_res = 0
    best_url = ''
    
    candidates = srcset.split(',')
    for candidate in candidates:
        parts = candidate.strip().split()
        if len(parts) != 2:
            continue
            
        url = parts[0]
        descriptor = parts[1]
        
        if descriptor.endswith('w'):
            res = int(descriptor[:-1])
            if res > highest_res:
                highest_res = res
                best_url = url
    
    # EÄŸer hiÃ§ 'w' tanÄ±mlayÄ±cÄ± bulunamazsa, ilk URL'yi al
    if not best_url and candidates:
        best_url = candidates[0].strip().split()[0]
        
    return best_url

def make_request_with_retry(url, max_retries=3, base_delay=2):
    """Rate limiting ile karÅŸÄ±laÅŸtÄ±ÄŸÄ±nda retry yapan request fonksiyonu"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'tr-TR,tr;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    for attempt in range(max_retries):
        try:
            # Random delay ekle (1-3 saniye arasÄ±)
            if attempt > 0:
                delay = base_delay * (2 ** attempt) + random.uniform(0, 2)
                print(f"Retry {attempt + 1}/{max_retries} - {delay:.1f} saniye bekleniyor...")
                time.sleep(delay)
            
            response = requests.get(url, headers=headers, timeout=30)
            
            # 429 hatasÄ± iÃ§in Ã¶zel kontrol
            if response.status_code == 429:
                if attempt < max_retries - 1:
                    print(f"Rate limit (429) - {base_delay * (2 ** (attempt + 1))} saniye bekleniyor...")
                    continue
                else:
                    raise requests.exceptions.HTTPError(f"429 Client Error: Too Many Requests - {max_retries} deneme sonrasÄ± baÅŸarÄ±sÄ±z")
            
            # DiÄŸer HTTP hatalarÄ±
            response.raise_for_status()
            return response
            
        except requests.exceptions.RequestException as e:
            if attempt == max_retries - 1:
                raise e
            print(f"Request hatasÄ± (deneme {attempt + 1}/{max_retries}): {e}")
            continue
    
    raise requests.exceptions.RequestException(f"TÃ¼m denemeler baÅŸarÄ±sÄ±z oldu")

@app.route('/api/scrape-url', methods=['POST'])
def scrape_url():
    data = request.json
    url = data.get('url')
    if not url:
        return jsonify({'error': 'URL is required'}), 400

    try:
        print(f"URL scraping baÅŸlatÄ±lÄ±yor: {url}")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en;q=0.8',
        }
        response = requests.get(url, headers=headers, timeout=30)
        print(f"Response status: {response.status_code}")
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # --- Temel Bilgiler ---
        title = soup.find('title').get_text(strip=True) if soup.find('title') else None
        description_meta = soup.find('meta', attrs={'name': 'description'})
        og_description = soup.find('meta', property='og:description')
        og_title = soup.find('meta', property='og:title')
        og_image = soup.find('meta', property='og:image')
        icon_link = soup.find('link', rel='icon') or soup.find('link', rel='shortcut icon')

        app_name = og_title['content'] if og_title and og_title.get('content') else title
        
        # --- GeliÅŸmiÅŸ AÃ§Ä±klama Ã‡ekme ---
        app_description = None
        
        # App Store iÃ§in Ã¶zel aÃ§Ä±klama Ã§ekme
        if 'itunes.apple.com' in url:
            print("App Store aÃ§Ä±klamasÄ± aranÄ±yor...")
            
            # 1. Ã–nce section elementini ara (en detaylÄ± aÃ§Ä±klama)
            description_section = soup.find('section', attrs={'data-test': 'product-description'})
            if description_section:
                # section iÃ§indeki tÃ¼m paragraflarÄ± al
                paragraphs = description_section.find_all('p')
                if paragraphs:
                    description_texts = []
                    for p in paragraphs:
                        text = p.get_text(strip=True)
                        if text and len(text) > 10:  # Ã‡ok kÄ±sa metinleri filtrele
                            description_texts.append(text)
                    if description_texts:
                        app_description = '\n\n'.join(description_texts)
                        print(f"Section aÃ§Ä±klamasÄ± bulundu: {len(app_description)} karakter")
            
            # 1.5. EÄŸer section bulunamadÄ±ysa, div class="l-column" ara (App Store yeni yapÄ±sÄ±)
            if not app_description:
                l_columns = soup.find_all('div', class_='l-column')
                for column in l_columns:
                    paragraphs = column.find_all('p')
                    if paragraphs:
                        description_texts = []
                        for p in paragraphs:
                            text = p.get_text(strip=True)
                            if text and len(text) > 20:  # Daha uzun metinleri al
                                description_texts.append(text)
                        if description_texts and len('\n\n'.join(description_texts)) > 100:
                            app_description = '\n\n'.join(description_texts)
                            print(f"L-column aÃ§Ä±klamasÄ± bulundu: {len(app_description)} karakter")
                            break
            
            # 2. EÄŸer section bulunamadÄ±ysa, div class="we-truncate" ara
            if not app_description:
                truncate_div = soup.find('div', class_='we-truncate')
                if truncate_div:
                    # we-truncate iÃ§indeki tÃ¼m paragraflarÄ± al
                    paragraphs = truncate_div.find_all('p')
                    if paragraphs:
                        description_texts = []
                        for p in paragraphs:
                            text = p.get_text(strip=True)
                            if text and len(text) > 10:
                                description_texts.append(text)
                        if description_texts:
                            app_description = '\n\n'.join(description_texts)
                            print(f"Truncate aÃ§Ä±klamasÄ± bulundu: {len(app_description)} karakter")
            
            # 3. EÄŸer hala bulunamadÄ±ysa, genel paragraf arama
            if not app_description:
                # Uzun paragraflarÄ± ara
                all_paragraphs = soup.find_all('p')
                long_paragraphs = []
                for p in all_paragraphs:
                    text = p.get_text(strip=True)
                    if text and len(text) > 50:  # En az 50 karakter
                        long_paragraphs.append(text)
                
                if long_paragraphs:
                    # En uzun paragrafÄ± seÃ§
                    app_description = max(long_paragraphs, key=len)
                    print(f"Genel paragraf aÃ§Ä±klamasÄ± bulundu: {len(app_description)} karakter")
            
            # 4. App Store iÃ§in Ã¶zel meta description ara
            if not app_description:
                meta_desc = soup.find('meta', attrs={'name': 'description'})
                if meta_desc and meta_desc.get('content'):
                    app_description = meta_desc['content']
                    print(f"Meta description aÃ§Ä±klamasÄ± bulundu: {len(app_description)} karakter")
        
        # DiÄŸer siteler iÃ§in geliÅŸmiÅŸ aÃ§Ä±klama Ã§ekme
        if not app_description:
            print("Genel site aÃ§Ä±klamasÄ± aranÄ±yor...")
            
            # 1. Uzun paragraflarÄ± ara
            all_paragraphs = soup.find_all('p')
            long_paragraphs = []
            for p in all_paragraphs:
                text = p.get_text(strip=True)
                if text and len(text) > 100:  # En az 100 karakter
                    long_paragraphs.append(text)
            
            if long_paragraphs:
                # En uzun paragrafÄ± seÃ§
                app_description = max(long_paragraphs, key=len)
                print(f"Uzun paragraf aÃ§Ä±klamasÄ± bulundu: {len(app_description)} karakter")
            
            # 2. EÄŸer hala bulunamadÄ±ysa, div class'larÄ±nÄ± ara
            if not app_description:
                description_divs = soup.find_all('div', class_=lambda x: x and any(keyword in x.lower() for keyword in ['description', 'content', 'about', 'summary']))
                for div in description_divs:
                    text = div.get_text(strip=True)
                    if text and len(text) > 50:
                        app_description = text
                        print(f"Div aÃ§Ä±klamasÄ± bulundu: {len(app_description)} karakter")
                        break
            
            # 3. Son fallback - meta tag'ler
            if not app_description:
                app_description = (og_description['content'] if og_description and og_description.get('content')
                                   else description_meta['content'] if description_meta and description_meta.get('content') else None)
                if app_description:
                    print(f"Meta tag aÃ§Ä±klamasÄ± bulundu: {len(app_description)} karakter")
        
        # AÃ§Ä±klamayÄ± temizle ve optimize et
        if app_description:
            # Sadece fazla boÅŸluklarÄ± temizle, satÄ±r sonlarÄ±nÄ± koru
            import re
            app_description = re.sub(r'\s+', ' ', app_description)  # Ã‡oklu boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
            app_description = app_description.strip()  # BaÅŸta ve sonda boÅŸluklarÄ± temizle
            
            # Ã‡ok kÄ±sa aÃ§Ä±klamalarÄ± filtrele
            if len(app_description) < 20:
                app_description = None
                print("AÃ§Ä±klama Ã§ok kÄ±sa, atlandÄ±")
            else:
                # Maksimum uzunluk sÄ±nÄ±rÄ± (Ã§ok uzun aÃ§Ä±klamalarÄ± kÄ±rp)
                if len(app_description) > 10000:  # 2000'den 10000'e Ã§Ä±karÄ±ldÄ±
                    app_description = app_description[:10000] + "..."
                    print("AÃ§Ä±klama 10000 karakter ile sÄ±nÄ±rlandÄ±rÄ±ldÄ±")
        
        print(f"App Name: {app_name}")
        print(f"App Description Length: {len(app_description) if app_description else 0} karakter")
        if app_description:
            print(f"Description Preview: {app_description[:200]}...")
        
        image_url = og_image['content'] if og_image and og_image.get('content') else None
        if not image_url and icon_link and icon_link.get('href'):
            image_url = urljoin(url, icon_link['href'])

        # --- SÃ¼rÃ¼m ve Dosya Boyutu ---
        app_version = None
        
        # App Store iÃ§in spesifik sÃ¼rÃ¼m tespiti
        # whats-new__latest__version class'Ä±ndan al
        version_element = soup.find('p', class_='l-column small-6 medium-12 whats-new__latest__version')
        if version_element:
            app_version = version_element.get_text(strip=True)
            print(f"App Store sÃ¼rÃ¼m bulundu: {app_version}")
        else:
            # Fallback - genel sÃ¼rÃ¼m arama
            version_keywords = ['version', 'sÃ¼rÃ¼m', 'current version', 'v.', 'ver.']
            app_version = find_by_keyword(soup, version_keywords, 'div') or find_by_keyword(soup, version_keywords, 'span')
            print(f"Genel sÃ¼rÃ¼m arama sonucu: {app_version}")
        
        # App Store iÃ§in spesifik boyut tespiti
        app_file_size = None
        size_element = soup.find('dd', class_='information-list__item__definition', attrs={'aria-label': True})
        if size_element:
            aria_label = size_element.get('aria-label', '').lower()
            if 'megabyte' in aria_label or 'gigabyte' in aria_label:
                app_file_size = size_element.get_text(strip=True)
        
        # EÄŸer App Store formatÄ±nda bulunamadÄ±ysa, genel arama yap
        if not app_file_size:
            # Fallback - genel boyut arama
            size_keywords = ['size', 'boyut', 'file size', 'mb', 'gb', 'kb', 'megabyte', 'gigabyte']
            app_file_size = find_by_keyword(soup, size_keywords, 'div') or find_by_keyword(soup, size_keywords, 'span')
        
        # SÃ¼rÃ¼m formatÄ±nÄ± dÃ¼zelt - sadece versiyon numarasÄ±
        if app_version:
            import re
            # "SÃ¼rÃ¼m" kelimesini kaldÄ±r
            app_version = re.sub(r'^sÃ¼rÃ¼m\s*', '', app_version, flags=re.IGNORECASE).strip()
            version_match = re.search(r'(\d+(?:\.\d+)*)', app_version)
            if version_match:
                app_version = version_match.group(1)  # Sadece versiyon numarasÄ±
                print(f"SÃ¼rÃ¼m formatlandÄ±: {app_version}")
            else:
                app_version = app_version  # Orijinal deÄŸeri koru
                print(f"SÃ¼rÃ¼m varsayÄ±lan format: {app_version}")
        else:
            app_version = '1.0'  # VarsayÄ±lan sÃ¼rÃ¼m (sadece numara)

        # --- Kategori Tespiti ---
        app_category = 'Kategorisiz'
        
        # App Store iÃ§in spesifik kategori tespiti
        print("Kategori aranÄ±yor...")
        
        # 1. Ã–nce kategori linkini ara
        category_link = soup.find('a', class_='link', href=lambda x: x and 'itunes.apple.com/tr/genre/' in x)
        if category_link:
            app_category = category_link.get_text(strip=True)
            print(f"Kategori linkinden bulundu: {app_category}")
        else:
            # 2. Fallback - dd elementini ara
            category_elements = soup.find_all('dd', class_='information-list__item__definition')
            for category_element in category_elements:
                category_link = category_element.find('a', class_='link')
                if category_link and 'itunes.apple.com/tr/genre/' in category_link.get('href', ''):
                    app_category = category_link.get_text(strip=True)
                    print(f"DD elementinden kategori linki bulundu: {app_category}")
                    break
                else:
                    # EÄŸer link yoksa, dd elementinin iÃ§indeki metni al
                    text = category_element.get_text(strip=True)
                    if text and len(text) > 3 and text != 'Kategorisiz':
                        app_category = text
                        print(f"DD elementinden metin bulundu: {app_category}")
                        break
        
        # 3. Kategori eÅŸleÅŸtirmesi yap (App Store kategorilerini bizim kategorilerimize Ã§evir)
        category_mapping = {
            'Ãœretkenlik': 'Verimlilik',
            'Productivity': 'Verimlilik',
            'Business': 'Ä°ÅŸ',
            'Developer Tools': 'GeliÅŸtirici AraÃ§larÄ±',
            'Education': 'EÄŸitim',
            'Entertainment': 'EÄŸlence',
            'Finance': 'Finans',
            'Graphics & Design': 'Grafik ve TasarÄ±m',
            'Health & Fitness': 'SaÄŸlÄ±k ve Fitness',
            'Lifestyle': 'YaÅŸam TarzÄ±',
            'Medical': 'TÄ±p',
            'Music': 'MÃ¼zik',
            'News': 'Haberler',
            'Photo & Video': 'FotoÄŸraf ve Video',
            'Reference': 'Referans',
            'Shopping': 'AlÄ±ÅŸveriÅŸ',
            'Social Networking': 'Sosyal AÄŸ',
            'Sports': 'Spor',
            'Travel': 'Seyahat',
            'Utilities': 'YardÄ±mcÄ± Programlar',
            'Weather': 'Hava Durumu'
        }
        
        if app_category in category_mapping:
            app_category = category_mapping[app_category]
            print(f"Kategori eÅŸleÅŸtirildi: {app_category}")
        
        # 4. EÄŸer hala bulunamadÄ±ysa, genel arama yap
        if app_category == 'Kategorisiz':
            # Fallback - genel kategori arama
            # "Kategori" kelimesinden sonraki metni bul
            def find_category_after_keyword(soup, keyword):
                # Regex ile daha esnek arama
                import re
                
                # Pattern 1: "Kategori" kelimesi ve sonrasÄ±ndaki metin
                patterns = [
                    # <div>Kategori</div><div>YardÄ±mcÄ±lar</div>
                    r'<div[^>]*>.*?' + re.escape(keyword) + r'.*?</div>\s*<div[^>]*>([^<]+)</div>',
                    # <span>Kategori</span><span>YardÄ±mcÄ±lar</span>
                    r'<span[^>]*>.*?' + re.escape(keyword) + r'.*?</span>\s*<span[^>]*>([^<]+)</span>',
                    # <label>Kategori</label><div>YardÄ±mcÄ±lar</div>
                    r'<label[^>]*>.*?' + re.escape(keyword) + r'.*?</label>\s*<div[^>]*>([^<]+)</div>',
                    # <strong>Kategori</strong><span>YardÄ±mcÄ±lar</span>
                    r'<strong[^>]*>.*?' + re.escape(keyword) + r'.*?</strong>\s*<span[^>]*>([^<]+)</span>',
                    # <b>Kategori</b><div>YardÄ±mcÄ±lar</div>
                    r'<b[^>]*>.*?' + re.escape(keyword) + r'.*?</b>\s*<div[^>]*>([^<]+)</div>',
                    # AynÄ± satÄ±rda: Kategori: YardÄ±mcÄ±lar
                    r'<[^>]*>.*?' + re.escape(keyword) + r'[:\s]*([^<\n]+)',
                ]
                
                html_content = str(soup)
                for pattern in patterns:
                    try:
                        matches = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
                        for match in matches:
                            if match and match.strip() and match.strip().lower() != keyword.lower():
                                return match.strip()
                    except:
                        continue
                
                # HTML element arama (fallback)
                elements = soup.find_all(['div', 'span', 'label', 'strong', 'b', 'p', 'td', 'th'])
                for element in elements:
                    text = element.get_text(strip=True)
                    if keyword.lower() in text.lower():
                        # KardeÅŸ elementleri kontrol et
                        for sibling in element.find_next_siblings():
                            sibling_text = sibling.get_text(strip=True)
                            if sibling_text and sibling_text.lower() != keyword.lower():
                                return sibling_text
                        
                        # Parent'Ä±n kardeÅŸ elementlerini kontrol et
                        if element.parent:
                            for sibling in element.parent.find_next_siblings():
                                sibling_text = sibling.get_text(strip=True)
                                if sibling_text and sibling_text.lower() != keyword.lower():
                                    return sibling_text
                        
                        # AynÄ± parent iÃ§indeki diÄŸer elementleri kontrol et
                        if element.parent:
                            for child in element.parent.find_all(['div', 'span', 'a', 'p', 'td', 'th']):
                                if child != element:
                                    child_text = child.get_text(strip=True)
                                    if child_text and child_text.lower() != keyword.lower():
                                        return child_text
                return None
            
            # Basit kategori tespiti - "Kategori" kelimesinden sonraki metin
            def find_simple_category(soup, keyword):
                # TÃ¼m metni al ve "Kategori" kelimesini ara
                all_text = soup.get_text()
                lines = all_text.split('\n')
                
                for i, line in enumerate(lines):
                    if keyword.lower() in line.lower():
                        # Bu satÄ±rda "Kategori" var, sonraki satÄ±rlara bak
                        for j in range(i+1, min(i+3, len(lines))):  # Sonraki 2 satÄ±ra bak
                            next_line = lines[j].strip()
                            if next_line and next_line.lower() != keyword.lower():
                                return next_line
                return None
            
            # TÃ¼rkÃ§e "Kategori" kelimesi ile ara
            category_text = find_simple_category(soup, 'Kategori')
            if category_text:
                app_category = category_text
            else:
                # Ä°ngilizce "Category" kelimesi ile ara
                category_text = find_simple_category(soup, 'Category')
                if category_text:
                    app_category = category_text
                else:
                    # GeliÅŸmiÅŸ arama
                    category_text = find_category_after_keyword(soup, 'Kategori')
                    if category_text:
                        app_category = category_text
                    else:
                        category_text = find_category_after_keyword(soup, 'Category')
                        if category_text:
                            app_category = category_text
                        else:
                            # App Store kategorileri (fallback)
                            category_element = soup.find('a', {'data-testid': 'category-link'}) or soup.find('a', class_='category-link')
                            if category_element:
                                app_category = category_element.get_text(strip=True)
                            
                            # Google Play kategorileri (fallback)
                            if not category_element:
                                category_element = soup.find('a', {'href': lambda x: x and '/store/apps/category/' in x})
                                if category_element:
                                    app_category = category_element.get_text(strip=True)

        # --- Ek GÃ¶rseller (Screenshots) ---
        internal_images = []
        
        # App Store iÃ§in spesifik picture elementlerinden gÃ¶rselleri al
        print("App Store picture elementleri aranÄ±yor...")
        picture_elements = soup.find_all('picture', class_=lambda x: x and 'we-artwork' in x)
        print(f"Bulunan picture elementleri: {len(picture_elements)}")
        
        for picture in picture_elements:
            # source elementlerini kontrol et
            sources = picture.find_all('source', attrs={'srcset': True})
            for source in sources:
                srcset = source['srcset']
                if 'mzstatic.com' in srcset:
                    # En yÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ linki al
                    best_url = get_highest_res_from_srcset(srcset)
                    if best_url and 'mzstatic.com' in best_url:
                        # SÄ±kÄ± filtreleme - sadece gerÃ§ek uygulama screenshot'larÄ±nÄ± al
                        def is_valid_screenshot(url):
                            # Ä°kon URL'lerini filtrele
                            icon_keywords = ['AppIcon', 'icon', 'Icon', 'logo', 'Logo']
                            if any(keyword in url for keyword in icon_keywords):
                                return False
                            
                            # Sadece belirli boyutlardaki gÃ¶rselleri al (screenshot boyutlarÄ±)
                            size_patterns = ['1286x0w', '1280x0w', '1200x0w', '1000x0w', '800x0w', '600x0w']
                            if not any(pattern in url for pattern in size_patterns):
                                return False
                            
                            # Sadece belirli dosya adÄ± pattern'lerini al
                            valid_patterns = [
                                'Hero', 'Screenshot', 'screenshot', 'App_Store_Image', 
                                'PurpleSource', 'Mac_', 'iPhone_', 'iPad_'
                            ]
                            if not any(pattern in url for pattern in valid_patterns):
                                return False
                            
                            return True
                        
                        if is_valid_screenshot(best_url):
                            # Duplikat kontrolÃ¼ - aynÄ± gÃ¶rselin farklÄ± formatlarÄ±nÄ± filtrele
                            # URL'den dosya adÄ±nÄ± Ã§Ä±kar (uzantÄ± olmadan)
                            import re
                            base_url = re.sub(r'\.(webp|png|jpg|jpeg)$', '', best_url)
                            
                            # Bu base URL zaten var mÄ± kontrol et
                            is_duplicate = False
                            for existing_url in internal_images:
                                existing_base = re.sub(r'\.(webp|png|jpg|jpeg)$', '', existing_url)
                                if base_url == existing_base:
                                    is_duplicate = True
                                    break
                            
                            if not is_duplicate:
                                internal_images.append(best_url)
                                print(f"GeÃ§erli screenshot eklendi: {best_url}")
                            else:
                                print(f"Duplikat gÃ¶rsel atlandÄ±: {best_url}")
                        else:
                            print(f"GeÃ§ersiz gÃ¶rsel atlandÄ±: {best_url}")
        
        # EÄŸer picture elementlerinden gÃ¶rsel bulunamadÄ±ysa, genel arama yap
        if not internal_images:
            print("Picture elementlerinden gÃ¶rsel bulunamadÄ±, genel arama yapÄ±lÄ±yor...")
            internal_images = extract_mzstatic_images(soup)
        
        # App Store iÃ§in Ã¶zel seÃ§iciler - mzstatic.com linklerini Ã¶ncelikle al
        def extract_mzstatic_images(soup):
            images = []
            
            # Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ olan mzstatic.com linklerini bul
            def is_screenshot_url(url):
                # Ä°kon URL'lerini filtrele
                icon_keywords = ['AppIcon', 'icon', 'Icon']
                screenshot_keywords = ['App_Store_Image', 'screenshot', 'Screenshot', 'PurpleSource']
                
                # Ä°kon URL'lerini hariÃ§ tut
                if any(keyword in url for keyword in icon_keywords):
                    return False
                
                # Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ URL'lerini al
                if any(keyword in url for keyword in screenshot_keywords):
                    return True
                
                # Genel resim URL'leri (ikonsuz)
                if 'mzstatic.com' in url and any(ext in url for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                    return True
                
                return False
            
            # TÃ¼m mzstatic.com linklerini bul
            for link in soup.find_all('a', href=True):
                href = link['href']
                if is_screenshot_url(href):
                    images.append(href)
            
            # srcset iÃ§indeki mzstatic linklerini bul
            for source in soup.find_all('source', attrs={'srcset': True}):
                srcset = source['srcset']
                if 'mzstatic.com' in srcset:
                    # En yÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ linki al
                    best_url = get_highest_res_from_srcset(srcset)
                    if best_url and is_screenshot_url(best_url):
                        images.append(best_url)
            
            # img src'lerde mzstatic linklerini bul
            for img in soup.find_all('img', src=True):
                src = img['src']
                if is_screenshot_url(src):
                    images.append(src)
            
            return list(set(images))  # DuplikatlarÄ± kaldÄ±r
        
        print(f"Toplam {len(internal_images)} uygulama iÃ§i gÃ¶rsel bulundu")

        # Google Play iÃ§in Ã¶zel seÃ§iciler
        if not internal_images:
            for img in soup.select('.screenshot img, .screenshot-image img'):
                src = img.get('src') or img.get('data-src')
                if src and src not in internal_images:
                    internal_images.append(urljoin(url, src))

        # Genel fallback - daha kapsamlÄ± resim arama
        if not internal_images:
            # FarklÄ± seÃ§iciler dene
            selectors = [
                '.screenshots img', 
                '.screenshot img', 
                '.app-screenshot img',
                '.gallery img',
                '.carousel img',
                '.slider img',
                '[data-screenshot] img',
                '.preview img'
            ]
            
            for selector in selectors:
                for img in soup.select(selector):
                    src = img.get('src') or img.get('data-src') or img.get('data-lazy')
                    if src and src not in internal_images and not src.startswith('data:'):
                        # Sadece gerÃ§ek URL'leri al
                        if src.startswith('http') or src.startswith('//'):
                            internal_images.append(urljoin(url, src))
                        elif src.startswith('/'):
                            internal_images.append(urljoin(url, src))
                
                if internal_images:  # EÄŸer resim bulunduysa dur
                    break

        # Ana ikon iÃ§in daha iyi arama
        if not image_url:
            # FarklÄ± ikon seÃ§icileri dene
            icon_selectors = [
                'link[rel="apple-touch-icon"]',
                'link[rel="icon"]',
                'link[rel="shortcut icon"]',
                'meta[property="og:image"]',
                'meta[name="twitter:image"]',
                '.app-icon img',
                '.logo img',
                '.icon img'
            ]
            
            for selector in icon_selectors:
                element = soup.select_one(selector)
                if element:
                    if element.name == 'link':
                        icon_url = element.get('href')
                    elif element.name == 'meta':
                        icon_url = element.get('content')
                    elif element.name == 'img':
                        icon_url = element.get('src') or element.get('data-src')
                    
                    if icon_url and not icon_url.startswith('data:'):
                        image_url = urljoin(url, icon_url)
                        break

        # GÃ¼ncelleme notlarÄ± tespiti
        update_notes = []
        
        # App Store gÃ¼ncelleme notlarÄ± iÃ§in spesifik tespit
        # Ã–nce whats-new ile baÅŸlayan div'leri ara
        whats_new_divs = soup.find_all('div', class_=lambda x: x and 'whats-new' in x)
        
        for whats_new_div in whats_new_divs:
            # Tarih ve sÃ¼rÃ¼m bilgisini al
            time_element = whats_new_div.find('time', attrs={'data-test-we-datetime': True})
            version_element = whats_new_div.find('p', class_=lambda x: x and 'version' in x)
            
            date_text = time_element.get_text(strip=True) if time_element else ''
            version_text = version_element.get_text(strip=True) if version_element else ''
            
            # GÃ¼ncelleme notlarÄ±nÄ± al - we-truncate ile baÅŸlayan div'leri ara
            content_divs = whats_new_div.find_all('div', class_=lambda x: x and 'we-truncate' in x)
            
            for content_div in content_divs:
                # TÃ¼m p elementlerini al
                paragraphs = content_div.find_all('p')
                
                for p in paragraphs:
                    text = p.get_text(strip=True)
                    if text and text not in ['', 'PATCH HIGHTLIGHTS']:
                        # BaÅŸlÄ±k olmayan metinleri al
                        if not text.isupper() or len(text) < 20:
                            update_notes.append(text)
                
                # EÄŸer gÃ¼ncelleme notlarÄ± bulunduysa dur
                if update_notes:
                    # EÄŸer tarih ve sÃ¼rÃ¼m varsa baÅŸa ekle
                    if date_text and version_text:
                        update_notes.insert(0, f"ğŸ“… {date_text} - {version_text}")
                    elif version_text:
                        update_notes.insert(0, f"ğŸ“± {version_text}")
                    break
            
            # EÄŸer gÃ¼ncelleme notlarÄ± bulunduysa dÃ¶ngÃ¼den Ã§Ä±k
            if update_notes:
                break

        
        # EÄŸer App Store formatÄ±nda bulunamadÄ±ysa, genel arama yap
        if not update_notes:
            # Daha geniÅŸ arama - tÃ¼m p elementlerinde bullet point'leri ara
            all_paragraphs = soup.find_all('p')
            for p in all_paragraphs:
                text = p.get_text(strip=True)
                # Bullet point ile baÅŸlayan metinleri al
                if text and (text.startswith('â€¢') or text.startswith('-') or text.startswith('*')):
                    if len(text) > 10:  # Ã‡ok kÄ±sa metinleri filtrele
                        update_notes.append(text)
            
            # EÄŸer hala bulunamadÄ±ysa, "what's new" benzeri anahtar kelimeleri ara
            if not update_notes:
                update_keywords = [
                    'what\'s new', 'gÃ¼ncelleme', 'yenilikler', 'changelog', 'release notes',
                    'update notes', 'version notes', 'patch notes', 'yeni Ã¶zellikler'
                ]
                
                for keyword in update_keywords:
                    # Div iÃ§inde ara
                    update_element = soup.find('div', string=lambda text: text and keyword.lower() in text.lower())
                    if update_element:
                        # KardeÅŸ elementleri kontrol et
                        for sibling in update_element.find_next_siblings():
                            sibling_text = sibling.get_text(strip=True)
                            if sibling_text and len(sibling_text) > 10:
                                update_notes.append(sibling_text)
                                break
                        break
                    
                    # H2, H3 baÅŸlÄ±klarÄ±nda ara
                    update_element = soup.find(['h2', 'h3'], string=lambda text: text and keyword.lower() in text.lower())
                    if update_element:
                        # Sonraki elementleri kontrol et
                        for sibling in update_element.find_next_siblings():
                            sibling_text = sibling.get_text(strip=True)
                            if sibling_text and len(sibling_text) > 10:
                                update_notes.append(sibling_text)
                                break
                        break

        # Sistem gereksinimleri tespiti
        system_requirements = []
        
        # App Store iÃ§in spesifik sistem gereksinimleri tespiti
        # <dd class="information-list__item__definition"> iÃ§indeki <dl> yapÄ±sÄ±nÄ± ara
        info_definitions = soup.find_all('dd', class_='information-list__item__definition')
        
        for info_def in info_definitions:
            # Ä°Ã§indeki <dl> elementlerini kontrol et
            dl_elements = info_def.find_all('dl', class_='information-list__item__definition__item')
            
            for dl in dl_elements:
                # <dt> ve <dd> elementlerini al
                dt = dl.find('dt', class_='information-list__item__definition__item__term')
                dd = dl.find('dd', class_='information-list__item__definition__item__definition')
                
                if dt and dd:
                    term_text = dt.get_text(strip=True)
                    definition_text = dd.get_text(strip=True)
                    
                    # Platform terimlerini kontrol et
                    platform_terms = ['Mac', 'iPhone', 'iPad', 'iPod touch', 'Apple TV', 'Apple Watch']
                    
                    if any(platform in term_text for platform in platform_terms):
                        # Sistem gereksinimlerini temizle ve ekle
                        clean_requirement = definition_text.replace('&nbsp;', ' ').strip()
                        if clean_requirement and len(clean_requirement) > 5:
                            system_requirements.append(f"{term_text}: {clean_requirement}")
        
        # EÄŸer App Store formatÄ±nda bulunamadÄ±ysa, genel arama yap
        if not system_requirements:
            # Sistem gereksinimleri iÃ§in anahtar kelimeler
            requirements_keywords = [
                'system requirements', 'sistem gereksinimleri', 'minimum requirements',
                'gerekli sistem', 'platform', 'iÅŸletim sistemi', 'operating system',
                'compatibility', 'uyumluluk', 'requirements', 'gereksinimler'
            ]
            
            # Sistem gereksinimleri metnini bul
            for keyword in requirements_keywords:
                # Div iÃ§inde ara
                req_element = soup.find('div', string=lambda text: text and keyword.lower() in text.lower())
                if req_element:
                    # KardeÅŸ elementleri kontrol et
                    for sibling in req_element.find_next_siblings():
                        sibling_text = sibling.get_text(strip=True)
                        if sibling_text and len(sibling_text) > 10:
                            system_requirements.append(sibling_text)
                            break
                    break
                
                # Span iÃ§inde ara
                req_element = soup.find('span', string=lambda text: text and keyword.lower() in text.lower())
                if req_element:
                    for sibling in req_element.find_next_siblings():
                        sibling_text = sibling.get_text(strip=True)
                        if sibling_text and len(sibling_text) > 10:
                            system_requirements.append(sibling_text)
                            break
                    break
                
                # P iÃ§inde ara
                req_element = soup.find('p', string=lambda text: text and keyword.lower() in text.lower())
                if req_element:
                    req_text = req_element.get_text(strip=True)
                    if req_text and len(req_text) > 10:
                        system_requirements.append(req_text)
                        break
            
            # EÄŸer hala bulunamadÄ±ysa, genel bilgilerden Ã§Ä±karÄ±m yap
            if not system_requirements:
                # iOS/Android/Windows/macOS gibi platform bilgilerini ara
                platform_keywords = ['iOS', 'Android', 'Windows', 'macOS', 'Linux', 'Chrome OS']
                for keyword in platform_keywords:
                    platform_elements = soup.find_all(string=lambda text: text and keyword in text)
                    for element in platform_elements:
                        parent_text = element.parent.get_text(strip=True) if element.parent else ''
                        if len(parent_text) > 5 and len(parent_text) < 200:
                            system_requirements.append(parent_text)
                            break
                    if system_requirements:
                        break

        return jsonify({
            'name': app_name,
            'description': app_description,
            'website': url,
            'image': image_url,
            'version': app_version,
            'fileSize': app_file_size,
            'category': app_category,
            'internalImages': internal_images[:10],
            'systemRequirements': system_requirements[:5],  # En fazla 5 sistem gereksinimi
            'features': update_notes[:10]  # En fazla 10 gÃ¼ncelleme notu
        })

    except requests.exceptions.RequestException as e:
        return jsonify({'error': f'URL getirilemedi: {e}'}), 500
    except Exception as e:
        return jsonify({'error': f'Veri Ã§ekme sÄ±rasÄ±nda bir hata oluÅŸtu: {e}'}), 500

@app.route('/api/apps', methods=['GET'])
def get_apps():
    all_apps = load_apps()

    # Arama ve Filtreleme
    search_term = request.args.get('search', '').lower()
    category = request.args.get('category', 'TÃ¼mÃ¼')
    sort_by = request.args.get('sort', 'default')

    filtered_apps = all_apps

    # Kategoriye gÃ¶re filtrele
    if category != 'TÃ¼mÃ¼' and category != 'Favoriler':
        # Ana kategori kontrolÃ¼ - alt kategorilerdeki tÃ¼m uygulamalarÄ± dahil et
        if category in CATEGORY_HIERARCHY:
            # Ana kategori ise, alt kategorilerdeki tÃ¼m uygulamalarÄ± al
            subcategories = CATEGORY_HIERARCHY[category]['subcategories']
            filtered_apps = [
                app for app in filtered_apps
                if app.get('category') and (
                    app.get('category') == category or  # Ana kategori
                    app.get('category') in subcategories or  # Alt kategoriler
                    any(app.get('category').startswith(sub + ' > ') for sub in subcategories)  # Alt kategori hiyerarÅŸisi
                )
            ]
        else:
            # Normal kategori filtreleme
            filtered_apps = [
                app for app in filtered_apps
                if app.get('category') and 
                   (app.get('category') == category or app.get('category').startswith(category + ' > '))
            ]

    # Arama terimine gÃ¶re filtrele
    if search_term:
        filtered_apps = [
            app for app in filtered_apps 
            if search_term in app.get('name', '').lower() or 
               search_term in app.get('description', '').lower()
        ]

    # SÄ±ralama
    if sort_by == 'name-asc':
        filtered_apps.sort(key=lambda x: x.get('name', ''))
    elif sort_by == 'name-desc':
        filtered_apps.sort(key=lambda x: x.get('name', ''), reverse=True)
    else: # 'default' (sadece tarih/saat sÄ±ralamasÄ±)
        def get_sort_priority(app):
            creation_date_str = app.get('creationDate')
            last_modified = datetime.fromisoformat(app.get('lastModified', '1970-01-01T00:00:00+00:00').replace('Z', '+00:00'))
            
            # Sadece tarih/saat sÄ±ralamasÄ± - rozet Ã¶nceliÄŸi yok
            if creation_date_str:
                creation_date = datetime.fromisoformat(creation_date_str.replace('Z', '+00:00'))
                # En yeni tarih/saat solda (ilk sÄ±rada) - creationDate ve lastModified'den daha yeni olanÄ± kullan
                return -max(creation_date.timestamp(), last_modified.timestamp())
            else:
                # creationDate yoksa sadece lastModified kullan
                return -last_modified.timestamp()
            
        filtered_apps.sort(key=get_sort_priority)

    # Sayfalama
    page = request.args.get('page', 1, type=int)
    limit = request.args.get('limit', 12, type=int)

    # limit=0 ise tÃ¼m sonuÃ§larÄ± dÃ¶ndÃ¼r, sayfalama yapma
    if limit == 0:
        paginated_apps = filtered_apps
        total_pages = 1
        page = 1
        total_apps = len(filtered_apps)
    else:
        total_apps = len(filtered_apps)
        total_pages = ceil(total_apps / limit)
        start_index = (page - 1) * limit
        end_index = start_index + limit
        paginated_apps = filtered_apps[start_index:end_index]

    return jsonify({
        'apps': paginated_apps,
        'totalPages': total_pages,
        'currentPage': page,
        'totalApps': total_apps
    })

@app.route('/api/apps', methods=['POST'])
def add_app():
    apps = load_apps()
    new_app_data = request.json
    
    # AynÄ± isimde uygulama var mÄ± kontrol et
    if any(app['name'] == new_app_data.get('name') for app in apps):
        return jsonify({'error': 'Bu isimde bir uygulama zaten mevcut'}), 409

    # SÃ¼rÃ¼m formatÄ±nÄ± dÃ¼zelt - sadece versiyon numarasÄ±
    if 'version' in new_app_data and new_app_data['version']:
        import re
        version = new_app_data['version']
        # "SÃ¼rÃ¼m" kelimesini kaldÄ±r
        version = re.sub(r'^sÃ¼rÃ¼m\s*', '', version, flags=re.IGNORECASE).strip()
        version_match = re.search(r'(\d+(?:\.\d+)*)', version)
        if version_match:
            new_app_data['version'] = version_match.group(1)  # Sadece versiyon numarasÄ±
        else:
            new_app_data['version'] = version  # Orijinal deÄŸeri koru

    now = datetime.now(timezone.utc).isoformat()
    new_app_data['creationDate'] = now
    new_app_data['lastModified'] = now
    if not 'reviews' in new_app_data:
        new_app_data['reviews'] = []
    if not 'badgeType' in new_app_data:
        new_app_data['badgeType'] = 'new'  # Yeni eklenen uygulamalar otomatik olarak "YENÄ°" rozeti alÄ±r

    apps.insert(0, new_app_data) # Yeni uygulamayÄ± listenin baÅŸÄ±na ekle
    save_apps(apps)
    return jsonify(new_app_data), 201

@app.route('/api/apps/<app_name>', methods=['PUT'])
def update_app(app_name):
    apps = load_apps()
    app_to_update = None
    app_index = -1

    for i, app in enumerate(apps):
        # Normalize both names for comparison - remove invisible characters
        app_name_normalized = app_name.strip().replace('\u200e', '').replace('\u200f', '')
        stored_name_normalized = app.get('name', '').strip().replace('\u200e', '').replace('\u200f', '')
        
        if stored_name_normalized == app_name_normalized:
            app_to_update = app
            app_index = i
            break

    if not app_to_update:
        return jsonify({'error': 'Uygulama bulunamadÄ±'}), 404

    update_data = request.json
    
    # Ä°sim deÄŸiÅŸikliÄŸi varsa, yeni ismin baÅŸka bir uygulamada kullanÄ±lmadÄ±ÄŸÄ±ndan emin ol
    new_name = update_data.get('name')
    if new_name != app_name and any(app['name'] == new_name for app in apps):
        return jsonify({'error': 'Bu isimde bir uygulama zaten mevcut'}), 409

    # Verileri gÃ¼ncelle
    update_data['lastModified'] = datetime.now(timezone.utc).isoformat()
    apps[app_index] = update_data
    
    save_apps(apps)
    return jsonify(apps[app_index])

@app.route('/api/apps/<app_name>', methods=['DELETE'])
def delete_app(app_name):
    apps = load_apps()
    original_length = len(apps)
    
    # Normalize app_name for comparison
    app_name_normalized = app_name.strip().replace('\u200e', '').replace('\u200f', '')
    apps_to_keep = [app for app in apps if app.get('name', '').strip().replace('\u200e', '').replace('\u200f', '') != app_name_normalized]

    if len(apps_to_keep) == original_length:
        return jsonify({'error': 'Uygulama bulunamadÄ±'}), 404

    save_apps(apps_to_keep)
    return jsonify({'message': f'{app_name} baÅŸarÄ±yla silindi'}), 200

@app.route('/api/featured-apps', methods=['GET'])
def get_featured_apps():
    """Ã–ne Ã§Ä±karÄ±lan uygulamalarÄ± getir"""
    all_apps = load_apps()
    featured_apps = [app for app in all_apps if app.get('featured', False)]
    
    # Ã–ne Ã§Ä±karÄ±lan uygulamalarÄ± tarihe gÃ¶re sÄ±rala (en yeni Ã¶nce)
    featured_apps.sort(key=lambda x: x.get('lastModified', '1970-01-01T00:00:00+00:00'), reverse=True)
    
    return jsonify(featured_apps)

@app.route('/api/apps/<app_name>/featured', methods=['PUT'])
def toggle_featured(app_name):
    """UygulamanÄ±n Ã¶ne Ã§Ä±karÄ±lan durumunu deÄŸiÅŸtir"""
    apps = load_apps()
    app_to_update = None
    app_index = -1


    for i, app in enumerate(apps):
        # Normalize both names for comparison - remove invisible characters
        app_name_normalized = app_name.strip().replace('\u200e', '').replace('\u200f', '')
        stored_name_normalized = app.get('name', '').strip().replace('\u200e', '').replace('\u200f', '')
        
        if stored_name_normalized == app_name_normalized:
            app_to_update = app
            app_index = i
            break

    if not app_to_update:
        return jsonify({'error': f'Uygulama bulunamadÄ±: {app_name}'}), 404

    # Featured durumunu toggle et
    current_featured = app_to_update.get('featured', False)
    apps[app_index]['featured'] = not current_featured
    
    # GÃ¼ncelleme tarihini gÃ¼ncelle
    apps[app_index]['lastModified'] = datetime.now(timezone.utc).isoformat()

    save_apps(apps)
    
    return jsonify({
        'message': f'Uygulama {"Ã¶ne Ã§Ä±karÄ±ldÄ±" if not current_featured else "Ã¶ne Ã§Ä±karÄ±lanlardan Ã§Ä±karÄ±ldÄ±"}',
        'featured': not current_featured
    })

# Kategori YÃ¶netimi Endpoint'leri

@app.route('/api/categories', methods=['GET'])
def get_categories():
    apps = load_apps()
    # Mevcut kategorileri al
    existing_categories = sorted(list(set(app.get('category') for app in apps if app.get('category'))))
    
    # HiyerarÅŸik yapÄ±yÄ± oluÅŸtur
    hierarchical_categories = {
        "hierarchy": CATEGORY_HIERARCHY,
        "existing": existing_categories,
        "all": get_all_categories()
    }
    
    return jsonify(hierarchical_categories)

@app.route('/api/categories', methods=['PUT'])
def rename_category():
    data = request.json
    old_name = data.get('old_name')
    new_name = data.get('new_name')

    if not old_name or not new_name:
        return jsonify({'error': 'Eski ve yeni kategori isimleri gereklidir'}), 400

    apps = load_apps()

    existing_categories = set(app.get('category') for app in apps if app.get('category'))
    if new_name in existing_categories and new_name != old_name:
        return jsonify({"error": f"'{new_name}' adÄ±nda bir kategori zaten mevcut"}), 409

    updated_count = 0
    for app in apps:
        app_category = app.get('category')
        if app_category:
            if app_category == old_name:
                app['category'] = new_name
                app['lastModified'] = datetime.now(timezone.utc).isoformat()
                updated_count += 1
            elif app_category.startswith(old_name + ' > '):
                app['category'] = new_name + app_category[len(old_name):]
                app['lastModified'] = datetime.now(timezone.utc).isoformat()
                updated_count += 1
    
    if updated_count > 0:
        save_apps(apps)

    return jsonify({"message": f"'{old_name}' kategorisi ve alt kategorileri '{new_name}' olarak deÄŸiÅŸtirildi. {updated_count} uygulama gÃ¼ncellendi."})


@app.route('/api/categories/<string:name>', methods=['DELETE'])
def delete_category(name):
    apps = load_apps()
    updated_count = 0
    for app in apps:
        app_category = app.get('category')
        if app_category:
            if app_category == name or app_category.startswith(name + ' > '):
                app['category'] = 'Kategorisiz'
                app['lastModified'] = datetime.now(timezone.utc).isoformat()
                updated_count += 1

    if updated_count > 0:
        save_apps(apps)

    return jsonify({"message": f"'{name}' kategorisi ve tÃ¼m alt kategorileri silindi. {updated_count} uygulama 'Kategorisiz' olarak iÅŸaretlendi."})


# Static dosya servisi
@app.route('/')
def index():
    return send_from_directory('.', 'index.html')

@app.route('/<path:filename>')
def static_files(filename):
    return send_from_directory('.', filename)


if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5001))
    debug = os.environ.get('FLASK_ENV') == 'development'
    app.run(host='0.0.0.0', port=port, debug=debug)